{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d374b338-a3e7-471b-b551-97b6517cec66",
   "metadata": {},
   "source": [
    "# Automatický zpětný průchod v Pytorch: autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996bbe4-8353-419e-a0f7-3feea5572276",
   "metadata": {},
   "source": [
    "- Knihovna Pytorch umožňuje automatický výpočet gradientů (autograd) a a jejich zpětnou propagaci.\n",
    "- Pokud vytvoříme jakýkoliv `Tensor` s dodatečným parametrem `requires_grad=True`, knihovna si zapamatuje všechny operace s ním provedené.\n",
    "- Zkusme nejprve velmi jednoduchý příklad násobení dvou tensorů s `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e449b3-b10f-4e97-9f6e-faa26ff9c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2087e8-5464-4854-becd-20fcffc437c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor(2., requires_grad=True)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03dd5407-50d3-4ad3-8276-a3e355d537a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.tensor(3., requires_grad=True)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec1052b4-63f7-431d-a77e-c38e79aef9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6., grad_fn=<MulBackward0>), True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = u * v\n",
    "w, w.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998be2bb-051c-4232-81fb-b8322ce3ac6c",
   "metadata": {},
   "source": [
    "Výsledkem násobení $2 \\cdot 3$ je nepřekvapivě hodnota $6$. Můžeme si ale všimnout dvou věcí:\n",
    "- Výsledný tensor `w` má rovněž nastaveno `requires_grad=True`.\n",
    "- U tensoru se objevil atribut `grad_fn=<MulBackward0>`. Ten značí funkci, která se bude volat při zpětném průchodu tak, aby mohl být příchozí gradient propagován grafem dále až ke vstupům, kterými jsou v našem příkladu `u` a `v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aeffa4c-fcd9-4c93-b6d9-0f9bb3d945cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MulBackward0 at 0x2a35f9075b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ad5e1-b57e-40dd-92b8-9002d257a391",
   "metadata": {},
   "source": [
    "Tím, že si každý tensor skrze atribut `grad_fn` pamatuje, z jaké operace pochází, resp. kterou funkci má volat, pokud se k němu při zpětném průchodu dostane příchozí gradient, vzniká orientovaný výpočetní graf.\n",
    "- Uzly jsou reprezentované proměnnými (např. tensory `u`, `v` a `w`) či operacemi nad nimi (např. `MulBackward0`).\n",
    "- Hrany jsou definovány odkazy na své předky - např. `w.grad_fn` odkazuje na \"rodiče\" (parent) `MulBackward0` uzlu `w`. Tento parent je funkce, která umí převzít příchozí gradient a řetízkovým pravidlem propaguje dál na své vstupy. Své vlastní předky má `MulBackward0` uloženy v atributu `next_functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831e63f4-53fd-42a1-9b22-a35e9a46d9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x2a35f907df0>, 0),\n",
       " (<AccumulateGrad at 0x2a35f907220>, 0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e9f29-66b4-4f8f-9da7-140eca8ad3ca",
   "metadata": {},
   "source": [
    "Předkem `MulBackward0` jsou dvě `AccumulateGrad`, které značí přičtení (akumulace) gradientu k nějaké proměnné. Jelikož v našem případě je graf velmi jednoduchý, jedná se již o listy grafu `u` a `v`.\n",
    "\n",
    "Jak uvidíme dále, gradienty se nepřepisují, ale akumulují tak, aby vše odpovídalo pravidlům diferenciálu a matematické analýzy v případech, kdy uzel má více než jednoho potomka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "747e78e8-161d-4c8b-970b-08ccdb6bf3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2., requires_grad=True), True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_ = w.grad_fn.next_functions[0][0].variable\n",
    "u_, u_ is u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99becb2c-6960-4127-9660-20f80611efc0",
   "metadata": {},
   "source": [
    "Celá zpětná propagace tedy spočívá v tom, že se postupně od konce výpočtů, tj. např. uzlu `w`, až na začátek postupně volají `grad_fn` jednotlivých uzlů a akumulují gradienty jejich přímých rodičů. Spustí se metodou `backward()` na uzlu, který si vybereme, obvykle ten na konci grafu - v našem případě `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef32b410-b8f4-4f7d-ba12-ced9bfd9204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd92e0d-c852-4479-a29b-1f23d7b2fc5c",
   "metadata": {},
   "source": [
    "Nyní proběhla zpětná propagace a derivace, tj. gradienty na všechny uzly, u kterých bylo nastaveno `requires_grad=True`, se uložily do atributů `grad` jednotlivých proměnných."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97af23d4-a830-4352-84bd-15d956add0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(2.))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.grad, v.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69b457-53d2-4af8-86e8-33a8e8274c40",
   "metadata": {},
   "source": [
    "Výsledek dává smysl, protože $w = u \\cdot v$ a tedy $dw / du = v = 3$ a $dw / dv = u = 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab12376b-d7f7-4809-bc0b-694c2f0aff1c",
   "metadata": {},
   "source": [
    "# Složená funkce: příklad $z = (x_1 + ax_2)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d1c7fa-06ac-4cf5-be5d-2a852b22d774",
   "metadata": {},
   "source": [
    "Pro ověření zkusme příklad z přednášky\n",
    "$$z = (x_1 + ax_2)^2$$\n",
    "Pokud dosadíme hodnoty $x_1 = 1$, $a = 2$ a $x_2 = 3$, dostaneme $z = (1 + 2 \\cdot 3)^2 = 49$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddcca1fa-5c46-4313-9584-afb41fc0b287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor(1., requires_grad=True)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eee72b8a-cd7f-4a16-a551-6661235538fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(2., requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d2a16c9-7347-4b4e-b5ac-61e594912d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.tensor(3., requires_grad=True)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab6cdc7f-c0af-41ab-88d1-f5d5ebb7fa6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x1 + a * x2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6990ff30-396b-4809-9f72-20840e162ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(49., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y ** 2\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b31e8f-85cf-4b0a-9eaf-37be972a935e",
   "metadata": {},
   "source": [
    "PyTorch autograd si nyní pamatuje celou historii výpočtů od `x1`, `a`, a `x2` až po `z`. Pokud nyní na `z` zavoláme metodu `backward(dout)` s nějakým \"příchozím\" gradientem `dout` (defaultně je `None`), spustí se kompletní zpětná propagace celým výpočetním stromem až k \"listům\" `x1`, `a`, a `x2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc714afc-1c35-4575-8128-58bdd1f58bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0abb234-7ad7-4a2d-b907-8d39e3b508a6",
   "metadata": {},
   "source": [
    "Pro derivaci `z` podle `x1`, tj. pro gradient platí\n",
    "$$\\frac{dz}{dx_1} = 2(x_1 + ax_2)$$\n",
    "což po dosazení $x_1 = 1$, $a = 2$ a $x_2 = 3$ vychází\n",
    "$$\\frac{dz}{dx_1} = 2\\cdot(1 + 2 \\cdot 3) = 14$$\n",
    "Výsledek můžeme ověřit v PyTorchi nahlédnutím do atributu `grad` tensoru `x1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "370ad6d1-f9d9-4652-af0a-a071fd69ca29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a13f41-a7b3-43fd-8e6e-2ffe2acfb572",
   "metadata": {},
   "source": [
    "Ovšem pokud bychom se chtěli podívat na $dz / dy$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fe111d9-8113-485a-b009-ea4b703cfcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  y.grad\n"
     ]
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68009ad3-60c4-4096-b14d-055ec57a7618",
   "metadata": {},
   "source": [
    "`y.grad` je `None`, protože PyTorch z důvodů šetření paměti zahazuje mezivýpočty, tj. všechny gradienty, které netvoří list stromu. Pokud bychom ho přesto chtěli vidět, lze na `y` zavolat funkci `retain_grad()` a znovu spustit zpětný průchod. K tomu však musíme znovu vytvořit celý výpočetní graf, jelikož PyTorch po zavolání backpropu defaultně vyčistí jeho buffery (toto chování lze změnit argumentem `retain_graph` metody `backward`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f4293b2-6954-4a1d-ac98-e17a3a3fc7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor(1., requires_grad=True)\n",
    "a = torch.tensor(2., requires_grad=True)\n",
    "x2 = torch.tensor(3., requires_grad=True)\n",
    "y = x1 + a * x2\n",
    "y.retain_grad()\n",
    "z = y ** 2\n",
    "z.backward(retain_graph=True)\n",
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb4a6d-33d5-4eb8-a29c-97963c0e9e69",
   "metadata": {},
   "source": [
    "Jelikož $z = y^2$, pro gradient platí $dz / dy = 2y$, což po dosazení hodnoty $y = 7$ z dopředného průchodu znamená, že $dz / dy = 14$.\n",
    "\n",
    "Podívejme nyní na gradienty vůči všem proměnným v grafu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27c9c3f7-fbcb-49cf-a1b1-09652a370efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  x1.grad, a.grad, x2.grad, y.grad, z.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(14.), tensor(42.), tensor(28.), tensor(14.), None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad, a.grad, x2.grad, y.grad, z.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfedcc53-9ffb-41bf-9890-3f6801533b4c",
   "metadata": {},
   "source": [
    "# Akumulace gradientů: příklad $p = (x^2+1) \\cdot (x^2-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01d926-3a42-434f-8135-4ac804cbfc2f",
   "metadata": {},
   "source": [
    "Poslední příklad ilustruje akumulaci gradientů v případě, kdy má jeden uzel grafu více potomků či ekvivalentně je předkem pro více uzlů.\n",
    "$$s = x^2$$\n",
    "$$p = s+1$$\n",
    "$$m = s-1$$\n",
    "$$q = p \\cdot m$$\n",
    "V takovém případě může vzniknout \"diamantový\" tvar grafu, protože na začátku je pouze jeden uzel `x`, který má dva potomky: uzly `p` a `m` (skrze `s`). Výsledek na konci, uzel `q`, pak opět své předky `p` a `m` spouje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e203ba1-4a45-4f4f-965a-f1360187eafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4., grad_fn=<PowBackward0>),\n",
       " tensor(5., grad_fn=<AddBackward0>),\n",
       " tensor(3., grad_fn=<SubBackward0>),\n",
       " tensor(15., grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2., requires_grad=True)\n",
    "\n",
    "s = x ** 2; s.retain_grad()\n",
    "p = s + 1; p.retain_grad()\n",
    "m = s - 1; m.retain_grad()\n",
    "q = p * m\n",
    "s, p, m, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bece6-0e89-447d-8280-be213218d346",
   "metadata": {},
   "source": [
    "Lokální derivace funkce jsou:\n",
    "$$\\frac{ds}{dx} = 2x$$\n",
    "$$\\frac{dp}{ds} = 1$$\n",
    "$$\\frac{dm}{ds} = 1$$\n",
    "$$\\frac{dq}{dp} = m$$\n",
    "$$\\frac{dq}{dm} = p$$\n",
    "\n",
    "Spusťme nyní zpětnou propagaci z uzlu `q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8d7ec19-6db6-4027-a1d7-5b6f57a50895",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4a07b-12ae-4095-a524-75c93afae7c0",
   "metadata": {},
   "source": [
    "Uzel `s` \"uvidí\" příchozí gradienty ze dvou větví, tj. z uzlů `p` a `m`, jejichž gradienty jsou:\n",
    "$$\\frac{dq}{dp} = m = 3$$\n",
    "$$\\frac{dq}{dm} = p = 5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40ce9d0e-3e95-4fc1-80b3-3f6ed0ff6cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(5.))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.grad, m.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2df5f8-e886-4635-ac05-fbae025949c3",
   "metadata": {},
   "source": [
    "V takovém případě je celková derivace dána jejich součtem:\n",
    "$$\\frac{dq}{ds} = \\frac{dq}{dp}\\frac{dp}{ds} + \\frac{dq}{dm}\\frac{dm}{dy} = 3 \\cdot 1 + 5 \\cdot 1 = 8$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07537016-8b50-480b-8826-012eb51592b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56e864-be12-49c1-9542-6c83b73adb88",
   "metadata": {},
   "source": [
    "\n",
    "Toto je důvod, proč PyTorch gradienty nepřepisuje, ale akumuluje. Při zpětné propagaci dochází k updatu `y.grad` dvakrát: jednou z větve `p` a jednou z větve `m`. Pokud by se gradient přepsal, jedna větev by \"vyhrála\" tím, že by updatovala jako poslední, a výsledný gradient by tak byl buď 3 nebo 5, což by ani v jednom případě nebyl správný výsledek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04e4e0f7-6a4d-48bf-805b-90b50c40d9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43bbbab-f7ec-4465-af9d-ab7ad4c8d61d",
   "metadata": {},
   "source": [
    "# `zero_grad()`\n",
    "\n",
    "Pro zajímavost: co se stane, pokud znovu zavoláme zpětný průchod `z.backward` z minuého příkladu (proměnné stále existují ve workspace)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89c87846-f751-486a-b282-5a7f0c921f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  x1.grad, a.grad, x2.grad, y.grad, z.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(14.), tensor(42.), tensor(28.), tensor(14.), None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad, a.grad, x2.grad, y.grad, z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0078c92-f210-4559-b3ca-217cc43a0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "defda9d0-c614-4ade-9656-855bb878c679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  x1.grad, a.grad, x2.grad, y.grad, z.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(28.), tensor(84.), tensor(56.), tensor(28.), None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad, a.grad, x2.grad, y.grad, z.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559380b7-7b24-4ad3-8dd7-caa3a0d4a6ac",
   "metadata": {},
   "source": [
    "Ačkoliv jsme s výpočetním grafem nic neprovedli, gradienty se změnily (zdvojnásobily)! Stalo se tak proto, že autograd atribut `grad` nepřepisuje, ale při každém backpropu akumuluje. Tensory `x1`, `a`, `x2`, `y` a `z` totiž nebyly znovu vytvořeny, nýbrž použity dvakrát a výsledné gradienty jsou proto *součtem* příchozích gradientů ze dvou pod-stromů. Pokud by PyTorch gradienty přepisoval, výsledek by neodpovídal pravidlům matematické analýzy.\n",
    "\n",
    "Pokud bychom vytvořili proměnné `x1`, `a`, `x2`, `y` a `z` znovu, reference na původní objekty by přestaly existovat, z prvního výpočetního grafu by nezbylo již nic a bylo by možné je garbage collectorem vyčistit z paměti. Dokud však proměnná s \"cachovaným\" `grad` existuje, není možné vyčistit paměť. Z grafového pohledu by `x1`, `a`, `x2`, `y` a `z` představovaly nové uzly s ještě prázdnými gradienty, které by se tak naplnily hodnotami shodnými s prvním průchodem.\n",
    "\n",
    "Toto chování má důležité implikace pro trénování sítí, kdy obodbným způsobem, jako v příkladu zacházíme s `x`, zacházíme s parametry sítě $W$ a $b$, např. když počítáme $s = Wx + b$. Mezi jednotlivými iteracemi SGD totiž $W$ a $b$ jako proměnné nevytváříme znovu, ale používáme stále stejné tensory. Tím vlastně dochází k opakovanému využití stejných uzlů, pokaždé ale vstupují do jiného grafu daného aktuální dávkou. Pokud před zavoláním `backprop()` tyto uzly mají nenulové gradienty `grad`, dojde k jejich akumulaci s hodnotami vypočtenými z minulé dávky. Nejenže nebudou odpovídat dopřednému průchodu na jedné dávce, ale zároveň mohou i nepříjemně narůstat. Mezi jednotlivými iteracemi je tedy nutné gradienty manuálně vynulovat, což se Pytorchi zařizuje metodou `zero_grad()` tříd `torch.nn.Module` nebo `torch.optim.Optimizer`. Nutnost manuálního volání `zero_grad()` v PyTorchi je jedním z nějčastějších zdrojů nepříjemných a špatně odhalitelných bugů.\n",
    "\n",
    "Podrobnější vysvětlení mechanismu automatického derivování, tzv. autogradu, lze pročíst např. na webu [pytorch.org](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ans24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
