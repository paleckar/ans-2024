{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dvouvrstv√Ω perceptron s vyu≈æit√≠m autogradu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- √ökolem cviƒçen√≠ je natr√©novat opƒõt **dvouvrstv√Ω perceptron** pro klasifikaci obr√°zk≈Ø na datasetu CIFAR-10, **tentokr√°t ov≈°em za pou≈æit√≠ modulu `ans.autograd` z minul√©ho cviƒçen√≠**.\n",
    "- Vytvo≈ô√≠me novou t≈ô√≠du `ans.classification.TwoLayerPerceptronAutograd`, kter√° bude obsahovat pouze dop≈ôedn√Ω pr≈Øchod. Zpƒõtn√Ω pr≈Øchod zajist√≠ automaticky implementace `ans.autograd.Variable.backprop` implementovan√° v minul√©m cviƒçen√≠.\n",
    "- Vytvo≈ô√≠me rovnƒõ≈æ nov√Ω modul `ans.nn`, do kter√©ho budeme postupnƒõ p≈ôid√°vat funkcionalitu souvisej√≠c√≠ s neuronov√Ωmi s√≠tƒõmi, abychom nemuseli neust√°le opakovat shodn√© bloky k√≥du.\n",
    "\n",
    "**Pozn√°mka**\n",
    "- Tento notebook nezap√≠n√° [autoreload extension](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html), proto≈æe testov√°n√≠ vyu≈æ√≠v√° type checking pomoc√≠ `isinstance` a [to p≈ôi reloadu modulu selh√°v√°](https://github.com/ipython/ipython/issues/12399).\n",
    "- P≈ôi ka≈æd√© modifikaci modulu `ans.autograd` je proto bohu≈æel nutn√© notebook restartovat a spustit cel√Ω k√≥d znovu (nap≈ô. \"Run All\") üòü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # import tests\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "import ans\n",
    "from tests import test_perceptron_autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P≈ô√≠prava diferencovateln√Ωch operac√≠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Z minul√©ho cviƒçen√≠ ma `Variable` implementovan√© pouze operace `+`, `-`, `*` a `/`.\n",
    "- Pro v√≠cevrstv√Ω perceptron (viz cviƒçen√≠ [two_layer_perceptron](two_layer_perceptron.ipynb)) optimalizovan√Ω softmax k≈ô√≠≈æovou entropi√≠ budeme je≈°tƒõ pot≈ôebovat alespo≈à\n",
    "  - maticov√© n√°soben√≠ `@` pro vnit≈ôn√≠ vrstvy perceptronu,\n",
    "  - nelinearitu `sigmoid`,\n",
    "  - `log`, `sum`, `exp` a `__getitem__` pro v√Ωpoƒçet lossu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maticov√© n√°soben√≠ `z = x @ y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Budeme uva≈æovat pouze minim√°lnƒõ dvourozmƒõrn√© tensory, tj. matice x matice a v√Ω≈°e.\n",
    "\n",
    "**Dop≈ôedn√Ω pr≈Øchod**\n",
    "\n",
    "$$\\boldsymbol{z} = \\boldsymbol{x} \\cdot \\boldsymbol{y}$$\n",
    "\n",
    "- $\\boldsymbol{x} = [x_{\\ldots,n,d}]$ je nejm√©nƒõ dvourozmƒõrn√Ω tensor s rozmƒõry $\\ldots \\times N \\times D$,\n",
    "- $\\boldsymbol{y} = [y_{\\ldots,d,k}]$ je nejm√©nƒõ dvourozmƒõrn√Ω tensor s rozmƒõry $\\ldots \\times D \\times K$,\n",
    "- $\\boldsymbol{z} = [z_{\\ldots,n,k}]$ je nejm√©nƒõ dvourozmƒõrn√Ω tensor s rozmƒõry $\\ldots \\times N \\times K$,\n",
    "\n",
    "**Zpƒõtn√Ω pr≈Øchod**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\overline{\\boldsymbol{x}} &= \\overline{\\boldsymbol{z}} \\cdot \\boldsymbol{y}^\\top \\\\\n",
    "    \\overline{\\boldsymbol{y}} &= \\boldsymbol{x}^\\top \\cdot \\overline{\\boldsymbol{z}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- transpozic√≠ $\\boldsymbol{x}^\\top$ pro v√≠cerozmƒõrn√© tensory se mysl√≠ prohozen√≠ posledn√≠ch dvou dimenz√≠ $\\ldots \\times N \\times D \\rightarrow \\ldots \\times D \\times N$\n",
    "\n",
    "**Bonus**\n",
    "\n",
    "- Zobecnƒõte tak aby oper√°tor `x @ y` fungoval i pro vektor (1D tensor) x matice a v≈°echny ostatn√≠ kombinace podporovan√© v PyTorchi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: implementujte oper√°tory [`ans.autograd.Variable.__matmul__`](../ans/autograd.py) a [`ans.autograd.Variable.__rmatmul__`](../ans/autograd.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestMatMul.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestMatMulGeneral.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid `z = x.sigmoid()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dop≈ôedn√Ω pr≈Øchod**\n",
    "\n",
    "$$z = \\frac{1}{1 + e^{-x}}$$\n",
    "- $x$ je re√°ln√© ƒç√≠slo (skal√°r)\n",
    "- $z$ je re√°lnƒõ ƒç√≠slo (skal√°r)\n",
    "\n",
    "**Zpƒõtn√Ω pr≈Øchod**\n",
    "\n",
    "Samostatnƒõ jako cviƒçen√≠ üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: implementujte metodu [`ans.autograd.Variable.sigmoid`](../ans/autograd.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestSigmoid.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umoc≈àov√°n√≠ `z = x.exp()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dop≈ôedn√Ω pr≈Øchod**\n",
    "\n",
    "$$\n",
    "z = e^{x}\n",
    "$$\n",
    "- $x$ je re√°ln√© ƒç√≠slo (skal√°r)\n",
    "- $z$ je re√°lnƒõ ƒç√≠slo (skal√°r)\n",
    "\n",
    "**Zpƒõtn√Ω pr≈Øchod**\n",
    "\n",
    "Samostatnƒõ jako cviƒçen√≠ üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: implementujte metodu [`ans.autograd.Variable.exp`](../ans/autograd.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestExp.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P≈ôirozen√Ω logaritmus `z = x.log()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nutn√© pro v√Ωpoƒçet k≈ô√≠≈æov√© entropie, kde se loss poƒç√≠t√° jako logaritmus z pravdƒõpodobnosti predikovan√© pro target t≈ô√≠du.\n",
    "\n",
    "**Dop≈ôedn√Ω pr≈Øchod**\n",
    "\n",
    "$$\n",
    "z = \\ln x\n",
    "$$\n",
    "- $x$ je re√°ln√© ƒç√≠slo (skal√°r)\n",
    "- $z$ je re√°lnƒõ ƒç√≠slo (skal√°r)\n",
    "\n",
    "**Zpƒõtn√Ω pr≈Øchod**\n",
    "\n",
    "Samostatnƒõ jako cviƒçen√≠ üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: implementujte metodu [`ans.autograd.Variable.log`](../ans/autograd.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestLog.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V√Ωbƒõr prvku `z = x[indices]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nutn√© pro v√Ωpoƒçet k≈ô√≠≈æov√© entropie, kde se loss poƒç√≠t√° jako $l = \\ln \\widehat{p}_y$, kde $\\widehat{p}_y$ je pravdƒõpodobnost predikovan√° modelem pro target t≈ô√≠du $y$.\n",
    "\n",
    "**Dop≈ôedn√Ω pr≈Øchod**\n",
    "\n",
    "- Operace je implementovan√° v PyTorch jako `z = x[ids]`, kde `ids` m≈Ø≈æe b√Ωt `int`, `tuple[int,...]`, `list[int,...]` nebo `torch.Tensor`.\n",
    "- Operaci si zjednodu≈°√≠me a v√Ωbƒõr omez√≠me na podmno≈æinu prvk≈Ø `x`, kde se nav√≠c nem≈Ø≈æe opakovat v√Ωbƒõr stejn√©ho prvku.\n",
    "- P≈ô√≠klad:\n",
    "  ``` python\n",
    "  x = torch.tensor([0.5, 0.6, 0.7])\n",
    "  z = x[[0]]  # z = torch.tensor([0.5]) ... ok\n",
    "  z = x[[0, 1]]  # z = torch.tensor([0.5, 0.6]) ... ok\n",
    "  z = x[[0, 0]]  # z = torch.tensor([0.5, 0.5]) ... validni v PyTorch, ale zde pro jednoduchost neuvazujeme (opakuje se prvek 0)\n",
    "  z = x[[False, True, True]]  # z = torch.tensor([0.6, 0.7]) ... ok\n",
    "  z = x[:2]  # z = torch.tensor([0.5, 0.6]) ... ok\n",
    "  ...\n",
    "  ```\n",
    "\n",
    "**Zpƒõtn√Ω pr≈Øchod**\n",
    "\n",
    "- Lok√°ln√≠ gradient na *vybran√©* prvky je jedna.\n",
    "- Lok√°ln√≠ gradient na *nevybran√©* prvky je nula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestGetItem.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Souƒçet prvk≈Ø `z = x.sum(dim=..., keepdim=...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nutn√© pro v√Ωpoƒçet softmaxu, kdy pot≈ôebujeme souƒçet p≈ôes pravdƒõpodobnosti t≈ô√≠d.\n",
    "\n",
    "**Dop≈ôedn√Ω pr≈Øchod**\n",
    "\n",
    "- Operace je implementovan√° v PyTorch jako `z = x.sum(...)`\n",
    "- Podporovan√© mus√≠ b√Ωt oba parametry `dim` a `keepdim`, viz <https://pytorch.org/docs/stable/generated/torch.sum.html>\n",
    "\n",
    "**Zpƒõtn√Ω pr≈Øchod**\n",
    "\n",
    "- Lok√°ln√≠ gradient na v≈°echny prvky je jedna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: implementujte metodu [`ans.autograd.Variable.sum`](../ans/autograd.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestSum.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus) Pr≈Ømƒõr prvk≈Ø `z = x.mean(dim=..., keepdim=...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podobn√© jako `x.sum(...)`, pouze s pr≈Ømƒõrem nam√≠sto souƒçtu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: implementujte metodu [`ans.autograd.Variable.mean`](../ans/autograd.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestMean.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T≈ô√≠da `TwoLayerPerceptronAutograd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C√≠lem cviƒçen√≠ je natr√©novat model dvouvrstv√©ho perceptronu bez explicitn√≠ho k√≥dov√°n√≠ zpƒõtn√©ho pr≈Øchodu, tj. s vyu≈æit√≠m autogradu.\n",
    "- Model bude imeplementovat t≈ô√≠da `ans.classification.TwoLayerPerceptronAutograd`.\n",
    "- Parametry klasifik√°toru budou reprezentov√°ny jako `ans.autograd.Variable`, viz n√°sleduj√≠c√≠ tabulku  \n",
    "  | atribut   | typ                     | znaƒçen√≠                                                            | rozmƒõr       |\n",
    "  |-----------|-------------------------|--------------------------------------------------------------------|--------------|\n",
    "  | `weight1` | `ans.autograd.Variable` | $\\boldsymbol{w}^{(1)} = \\left[w_{d,h}^{(1)}\\right]$                | $D \\times H$ |\n",
    "  | `bias1`   | `ans.autograd.Variable` | $\\boldsymbol{b}^{(1)} = \\left[b_1^{(1)}, \\ldots, b_H^{(1)}\\right]$ | $H$          |\n",
    "  | `weight2` | `ans.autograd.Variable` | $\\boldsymbol{w}^{(2)} = \\left[w_{d,k}^{(2)}\\right]$                | $H \\times K$ |\n",
    "  | `bias2`   | `ans.autograd.Variable` | $\\boldsymbol{b}^{(2)} = \\left[b_1^{(2)}, \\ldots, b_K^{(2)}\\right]$ | $K$          |\n",
    "- T≈ô√≠da bude obsahovat pouze k√≥d pro dop≈ôedn√Ω pr≈Øchod (pr≈Øchod s√≠t√≠ a v√Ωpoƒçet lossu).\n",
    "- Zpƒõtn√Ω pr≈Øchod bude vol√°n jako\n",
    "  ``` python\n",
    "  # vynulovani gradientu\n",
    "  ...\n",
    "  # zpetna propagace\n",
    "  loss.backprop()\n",
    "  ```\n",
    "- Po jeho proveden√≠ budou vyplnƒõny atributy `.grad` v≈°echny parametr≈Ø modelu, co≈æ umo≈æn√≠ update pomoc√≠ SGD.\n",
    "- **D≈Øle≈æit√© je p≈ôed ka≈æd√Ωm zpƒõtn√Ωm pr≈Øchodem vynulovat gradienty parametr≈Ø, aby nedoch√°zelo k jejich akumulaci p≈ôes r≈Øzn√© d√°vky (minibatche)**, viz nap≈ô. <https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch>.\n",
    "\n",
    "**Funkce `softmax_cross_entropy`**\n",
    "- V√Ωpoƒçet lossu implementujte jako funkci `ans.classification.softmax_cross_entropy`, aby se nemusely opakovat stejn√© kusy k√≥du v `TwoLayerPerceptronAutograd.train_step` a `TwoLayerPerceptronAutograd.val_Step`.\n",
    "- Funkce by mƒõla b√Ωt agnostick√° v≈Øƒçi typu `logits` parametru:\n",
    "  - pokud je `torch.Tensor`, v√Ωsledn√Ω loss bude rovnƒõ≈æ `torch.Tensor`,\n",
    "  - pokud je `Variable`, v√Ωsledn√Ω loss bude rovnƒõ≈æ `Variable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: implementuje funkci [`ans.classification.softmax_cross_entropy`](../ans/classification.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestSoftmaxCrossEntropy.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: implementuje metodu [`ans.classification.TwoLayerPerceptronAutograd.__init__`](../ans/classification.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestInit.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: implementuje funkci [`ans.classification.TwoLayerPerceptronAutograd.train_step`](../ans/classification.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestTrainStep.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: implementuje funkci [`ans.classification.TwoLayerPerceptronAutograd.val_step`](../ans/classification.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestValStep.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pro tr√©nov√°n√≠ bude pot≈ôeba opƒõt je≈°tƒõ preprocessing dat, podobnƒõ jako v minul√Ωch cviƒçen√≠ch.\n",
    "- Pou≈æijeme shodn√Ω preprocessing jako ve cviƒçen√≠ch [linear_classification](linear_classification.ipynb) a [two_two_layer_perceptron](two_layer_perceptron.ipynb), tj. zplo≈°ten√≠ obr√°zk≈Ø do vektor≈Ø a p≈ôevod hodnot pixel≈Ø 0..255 do rozsahu 0..1 prost√Ωm vydƒõlen√≠m 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: implementuje funkci `preprocess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img: PIL.Image) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: image of type PIL.Image\n",
    "    Returns:\n",
    "        x: 1-dimensional tensor; shape (num_pixels * num_channels,), dtype float32\n",
    "    \"\"\"\n",
    "    \n",
    "    ########################################\n",
    "    # TODO: implement\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "    # ENDTODO\n",
    "    ########################################\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestPreprocess.eval(preprocess_fn=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr√©nov√°n√≠ klasifik√°toru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pokud inicializujete parametry ve stejn√©m po≈ôad√≠ a shodn√Ωm zp≈Øsobem jako ve cviƒçen√≠ [two_two_layer_perceptron](two_layer_perceptron.ipynb) (a pouze \"obal√≠te\" do `Variable`), mƒõli byste dostat naprosto stejn√° ƒç√≠sla vƒçetnƒõ progressu lossu!\n",
    "- Nejlep≈°√≠ model ulo≈æte jako\n",
    "  ``` python\n",
    "  model.save('../output/two_layer_perceptron_autograd_weights.pt')\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Natr√©nujte dvouvrstv√Ω perceptron tak, aby dos√°hl alespo≈à 45 % (bonusovƒõ 50 %) *validaƒçn√≠* accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.utils.seed_everything(0)\n",
    "\n",
    "########################################\n",
    "# TODO: implement\n",
    "\n",
    "# ...\n",
    "# model = ans.classification.TwoLayerPerceptronAutograd(...)\n",
    "# ...\n",
    "\n",
    "raise NotImplementedError\n",
    "\n",
    "# ENDTODO\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestValAccuracy45.eval(preprocess_fn=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perceptron_autograd.TestValAccuracy50.eval(preprocess_fn=preprocess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ans24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
